<!DOCTYPE html>
<html>
  <head>
    <title>Sino-German Symposium GZ1607</title>
    <meta http-equiv="Pragma" content="no-cache">
	<meta http-equiv="Cache-Control" content="no-cache">
	<meta http-equiv="Expires" content="0">
    <link rel="stylesheet" href="styles/base.css">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <script src="js/slideshow.js"></script>
    <!---script src="js/better-simple-slideshow.min.js"></script--->

    <script src="js/jquery-3.5.1.js"></script>
  </head>
  <body>
	<div class="nav-container" id="navContainer"></div>
	<!---script>
		$("#navContainer").load("include/header.html");
	</script-->


    <nav class="nav">
      <a href="./">
        <img class="nav__logo" src="img/ucsd.svg" alt="Uni Bonn">
      </a>
      <span class="nav__menu" onClick="expandMenu()">&#9776;</span>
      <script>
        function expandMenu() {
          document.getElementById('navContainer').classList.toggle('expanded');
        }
      </script>
      <div class="nav__separator"></div>
      <a href="./" class="nav__item">Home</a>
      <a href="news.html" class="nav__item">News</a>
      <a href="teach.html" class="nav__item">Lectures</a>
      <a href="talks.html" class="nav__item">Talks</a>
      <a href="research.html" class="nav__item">Research</a>
      <a href="publications.html" class="nav__item">Publications</a>
      <!-- sub menu ------------------

              <a href="team.php" class="nav__item">People</a>
              <div class="nav-sub">
                  <a href="team.php" class="nav__item">People <small><small><small><small>&#9660</small></small></small></small></a>
                  <div class="nav-sub__item">
                      <a href="team.php">Lab members</a>
                      <a href="gallery.html">Gallery</a>
                  </div>
              </div>
      -------------------------------- -->
      <!--a href="openings.html" class="nav__item">Openings</a>
      <a href="intranet.html" class="nav__item">Resources</a>
      <a href="tour.html" class="nav__item">Lab Tour</a-->
      <a href="contact.html" class="nav__item">Contact</a>
    </nav>

    <!-- end of header -->
    
    <div class="hero">
      <div class="slideshow" id="slideshow">
        <img src="img/event/bjevt.png" alt="">
        <img src="img/event/bjroad.png" alt="">

        <img src="img/event/bjbo.png" alt="">
        <img src="img/event/bjvt.png" alt="">

        <img src="img/event/bjbtabc.png" alt="">
        <img src="img/event/bjjt.png" alt="">

        <img src="img/event/bjsb.png" alt="">
        <img src="img/event/bjron.png" alt="">

        <img src="img/event/bjam.png" alt="">
        <img src="img/event/bjar.png" alt="">

        <img src="img/event/bjabc.png" alt="">
        <img src="img/event/bjpascal.png" alt="">
        <img src="img/event/bjbt.png" alt="">
        <img src="img/event/bjsun.png" alt="">

      </div>
      <div class="slideshow__buttons" id="slideshow_buttons">
      </div>
      <div class="hero__text">
        <img src="img/hero-shapes.svg" alt="shapes" class="hero__shapes">
        <h1 class="hero__title">
          Sino-German Symposium <br class="text-divider"/>GZ1607
          <span class="hero__subtitle"> Symbolic Representation | Numeric Representation | Commonsense Reasoning </span>
        </h1>
      </div>
    </div>

    <section>
      <div class="journal-select">
        <span class="journal-select__text">Select:</span></br>
        <a href="#2021"><span data-block-id="2021" class="journal-select__item">October 22, 2019</span></a>
        <a href="#2020"><span data-block-id="2020" class="journal-select__item">October 23, 2019</span></a>
        <a href="#2019"><span data-block-id="2019" class="journal-select__item">October 24, 2019</span></a>
      </div>
    </section>

    </ul>
    <ol reversed start="35" id="October 22, 2019" class="publications-container">
      <h2 class="section__title">2021</h2>
      <div class="publications-section-container">
        <div class="publication-item">
          <span class="publication-item__title normal">08:30 - 08:50  Session 1: Opening Ceremony </span>
          <span class="publication-item__subtitle"><b class="publication-item__journal">Welcome Oration, Officers from NSFC-DFG, Prof. Maosong Sun, Prof. Armin B. Cremers</b>  Chair: Juanzi Li (Tsinghua University)
          </span>
        </div>

        <div class="publication-item">
          <span class="publication-item__title normal">08:50 - 09:40 	Session 2 </span>
          <span class="publication-item__subtitle"><b class="publication-item__journal">Keynote 'Thinking with the Mind and the World'</b>  Speaker: Barbara Tversky (Stanford University and Columbia University Teachers Colleges)
          <span class="publication-item__authors">Chair: Tiansi Dong (Universit√§t Bonn)</span>
          </span>
        </div>



      </div>
    </ol>
    <ol reversed start="33" id="October 23, 2019" class="publications-container">
      <h2 class="section__title">2020</h2>
      <div class="publications-section-container">
        <div class="publication-item"> <!--134-->
          <span class="publication-item__title normal">Learning Syllogism with Euler Neural-Networks </span>
          <span class="publication-item__subtitle"><b class="publication-item__journal">arXiv:2007.07320  </b> (2020)<a href="https://arxiv.org/abs/2007.07320" target="_blank">(link)</a></span>
          <span class="publication-item__authors"><b>Dong, Tiansi</b>, Chengjiang Li, Christian Bauckhage, Juanzi Li, Stefan Wrobel, Armin B. Cremers</span>
          <details>
            <summary class="publication-item__abstract"><b>Abstract</b> (click)</summary>
            <p class="publication-item__text">Traditional neural networks represent everything as a vector, and are able to approximate a subset of logical
              reasoning to a certain degree. As basic logic relations are better represented by topological relations between regions, we propose a novel
              neural network that represents everything as a ball and is able to learn topological configuration as an Euler diagram. So comes the name
              Euler Neural-Network (ENN). The central vector of a ball is a vector that can inherit representation power of traditional neural network.
              ENN distinguishes four spatial statuses between balls, namely, being disconnected, being partially overlapped, being part of, being inverse
              part of. Within each status, ideal values are defined for efficient reasoning. A novel back-propagation algorithm with six Rectified Spatial
              Units (ReSU) can optimize an Euler diagram representing logical premises, from which logical conclusion can be deduced. In contrast to
              traditional neural network, ENN can precisely represent all 24 different structures of Syllogism. Two large datasets are created: one extracted
              from WordNet-3.0 covers all types of Syllogism reasoning, the other extracted all family relations from DBpedia. Experiment results approve the
              superior power of ENN in logical representation and reasoning. Datasets and source code are available upon request. </p>
            <!--img class="publication-item__image" src="img/publications/journals/134.jpg" alt="google_scholar" /-->
          </details>
        </div>
      </div>
    </ol>
    <ol reversed start="32" id="October 24, 2019" class="publications-container">
      <h2 class="section__title">2019</h2>
      <div class="publications-section-container">
        <div class="publication-item"> <!--124-->
          <span class="publication-item__title normal">Triple Classification Using Regions and Fine-Grained Entity Typing.</span>
          <span class="publication-item__subtitle"><b class="publication-item__journal">AAAI</b> 2019<a href="https://www.researchgate.net/publication/328703147_Triple_Classification_Using_Regions_and_Fine-Grained_Entity_Typing" target="_blank">(link)</a>
              </span>
          <span class="publication-item__authors"><b>Tiansi Dong</b>, Zhigang Wang, Juanzi Li, Christian Bauckhage, Armin B. Cremers </span>
          <details>
            <summary class="publication-item__abstract"><b>Abstract</b> (click)</summary>
            <p class="publication-item__text">A Triple in knowledge-graph takes a form that consists of head, relation, tail. Triple Classification
              is used to determine the truth value of an unknown Triple. This is a hard task for 1-to-N relations using the vector-based embedding
              approach. We propose a new region-based embedding approach using fine-grained type chains. A novel geometric process is presented to
              extend the vectors of pre-trained entities into n-balls (n-dimensional balls) under the condition that head balls shall contain their
              tail balls. Our algorithm achieves zero energy loss, therefore, serves as a case study of perfectly imposing tree structures into
              vector space. An unknown Triple (h, r, x) will be predicted as true, when x's n-ball is located in the r-subspace of h's n-ball,
              following the same construction of known tails of h. The experiments are based on large datasets derived from the benchmark datasets
              WN11, FB13, and WN18. Our results show that the performance of the new method is related to the length of the type chain and the
              quality of pre-trained entity-embeddings, and that performances of long chains with well-trained entity-embeddings outperform other
              methods in the literature.</p>
            <!--img class="publication-item__image" src="img/publications/journals/124.png" alt="google_scholar" /-->
          </details>
        </div>
        <div class="publication-item"> <!--123-->
          <span class="publication-item__title normal">Imposing Category Trees Onto Word-Embeddings Using A Geometric Construction</span>
          <span class="publication-item__subtitle"><b class="publication-item__journal">ICLR</b> 2019<a href="https://www.researchgate.net/publication/331319652_Imposing_Category_Trees_Onto_Word-Embeddings_Using_A_Geometric_Construction" target="_blank">(link)</a>
              </span>
          <span class="publication-item__authors"><b>Tiansi Dong</b>, Christian Bauckhage, Hailong Jin, Juanzi Li, Olaf Cremers, Daniel Speicher, Armin B. Cremers, Joerg Zimmermann</span>
          <details>
            <summary class="publication-item__abstract"><b>Abstract</b> (click)</summary>
            <p class="publication-item__text">We present a novel method to precisely impose tree-structured category information onto word-embeddings,
              resulting in ball embeddings in higher dimensional spaces ($\mathcal{N}$-balls for short). Inclusion relations among $\mathcal{N}$-balls
              implicitly encode subordinate relations among categories. The similarity measurement in terms of the cosine function is enriched by
              category information. Using a geometric construction method instead of back-propagation, we create large $\mathcal{N}$-ball embeddings
              that satisfy two conditions: (1) category trees are precisely imposed onto word embeddings at zero energy cost; (2) pre-trained word
              embeddings are well preserved. A new benchmark data set is created for validating the category of unknown words. Experiments show that
              $\mathcal{N}$-ball embeddings, carrying category information, significantly outperform word embeddings in the test of nearest neighborhoods,
              and demonstrate surprisingly good performance in validating categories of unknown words. Source codes and data-sets are free for
              public access \url{https://github.com/GnodIsNait/nball4tree.git} and \url{https://github.com/GnodIsNait/bp94nball.git}.</p>
            <!--img class="publication-item__image" src="img/publications/journals/123.png" alt="google_scholar" /-->
          </details>
        </div>
        <div class="publication-item"> <!--122-->
          <span class="publication-item__title normal">Fine-Grained Entity Typing via Hierarchical Multi Graph Convolutional Networks</span>
          <span class="publication-item__subtitle"><b class="publication-item__journal">EMNLP-IJCNLP</b> 2019<a href="https://aclanthology.org/D19-1502/" target="_blank">(link)</a>
              </span>
          <span class="publication-item__authors">Jin HaiLong, Hou Lei, Li Juanzi, <b>Tiansi Dong</b></span>
          <details>
            <summary class="publication-item__abstract"><b>Abstract</b> (click)</summary>
            <p class="publication-item__text">This paper addresses the problem of inferring the fine-grained type of an entity from a
              knowledge base. We convert this problem into the task of graph-based semi-supervised classification, and propose
              Hierarchical Multi Graph Convolutional Network (HMGCN), a novel Deep Learning architecture to tackle this problem.
              We construct three kinds of connectivity matrices to capture different kinds of semantic correlations between entities.
              A recursive regularization is proposed to model the subClassOf relations between types in given type hierarchy.
              Extensive experiments with two large-scale public datasets show that our proposed method significantly outperforms
              four state-of-the-art methods.</p>
            <!--img class="publication-item__image" src="img/publications/journals/122.png" alt="google_scholar" /-->
          </details>
        </div>
        <div class="publication-item">  <!--121-->
          <span class="publication-item__title normal">Prototypes within Minimum Enclos- ing Balls</span>
          <span class="publication-item__subtitle"><b class="publication-item__journal">ICANN </b> 2019<a href=https://www.researchgate.net/publication/335730052_Prototypes_Within_Minimum_Enclosing_Balls target="_blank">(link)</a>
              </span>
          <span class="publication-item__authors">Christian Bauckhage, Rafet Sifa, and <b>Tiansi Dong</b></span>
          <details>
            <summary class="publication-item__abstract"><b>Abstract</b> (click)</summary>
            <p class="publication-item__text">We revisit the kernel minimum enclosing ball problem and show that it can be solved using
              simple recurrent neural networks. Once solved, the interior of a ball can be characterized in terms of a function of a set
              of support vectors and local minima of this function can be thought of as prototypes of the data at hand. For Gaussian
              kernels, these minima can be naturally found via a mean shift procedure and thus via another recurrent neurocomputing process.
              Practical results demonstrate that prototypes found this way are descriptive, meaningful, and interpretable.</p>
            <!--img class="publication-item__image" src="img/publications/journals/121.png" alt="google_scholar" /-->
          </details>
        </div>
      </div>
    </ol>

    </body>



</html>
